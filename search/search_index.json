{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p><code>walk:ai</code> is an open-source platform for running AI/ML workloads on Kubernetes in an intuitive and optimized way, focused on increasing GPU utilization.</p> <p>Currently, the platform consists of three componentes:</p> <ul> <li> <p>nos: a set of services deployed in your cluster that handle dynamic GPU partitioning and send GPU and workload telemetry back to the <code>walk:ai</code> application.</p> </li> <li> <p>Application: a single control plane to submit jobs, inspect runs, manage volumes and secrets, and observe cluster GPU usage through a browser-based UI.</p> </li> <li> <p>Command-line tool: a developer-friendly CLI that turns Python projects into container images without writing Dockerfiles or Kubernetes manifests. Also lets you interface with the <code>walk:ai</code> platform just like in the browser, but without leaving your terminal.</p> </li> </ul>"},{"location":"overview/","title":"Overview","text":"<p><code>walk:ai</code> is an open-source platform for running AI/ML workloads on Kubernetes in an intuitive and optimized way, focused on increasing GPU utilization.</p> <p>Currently, the platform consists of three componentes:</p> <ul> <li> <p>nos: a set of services deployed in your cluster that handle dynamic GPU partitioning and send GPU and workload telemetry back to the <code>walk:ai</code> application.</p> </li> <li> <p>Application: a single control plane to submit jobs, inspect runs, manage volumes and secrets, and observe cluster GPU usage through a browser-based UI.</p> </li> <li> <p>Command-line tool: a developer-friendly CLI that turns Python projects into container images without writing Dockerfiles or Kubernetes manifests. Also lets you interface with the <code>walk:ai</code> platform just like in the browser, but without leaving your terminal.</p> </li> </ul> <p></p>"},{"location":"vpn/","title":"Vpn","text":"<ol> <li>Crear resource group en region deseada</li> <li>Crear una virtual network en AZ, eligiendo su espacio de direcciones</li> <li>Crear una subnet, con un subespacio de direcciones.</li> <li>Crear otra subnet, de tipo Gateway, con otro subespacio, mas peque\u00f1o, pero no menor a /27. </li> <li> <p>Crear virtual network gateway</p> <ul> <li>Buscar en search, es la de marketplace.</li> <li>Hay que usar la subnet de tipo Gateway</li> </ul> </li> <li> <p>Crear VPC en AWS</p> <ul> <li>Crear subnets</li> <li>Crear route tables</li> </ul> </li> <li>Crear customer gateway en AWS utilizando la IP p\u00fablica de la virtual network gateway previa.</li> <li>Crear Virtual Private Gateway<ul> <li>Attach a la VPC creada previamente.</li> </ul> </li> <li>Crear Site-to-Site VPN<ul> <li>Tenes que asignarle la customer gateway de (7)</li> <li>La Virtual Private Gateway de (8)</li> <li>En static routes hay que poner la subnet de (3) (subnet de la VM)</li> </ul> </li> </ol>"},{"location":"app/configuration/","title":"WIP","text":""},{"location":"app/installation/","title":"WIP","text":""},{"location":"app/overview/","title":"Application","text":"<p>The <code>walk:ai application</code> is the control plane of the platform. It exposes an HTTP API and a browser-based console to submit and monitor jobs, manage data, and inspect how GPUs are being used across the cluster.</p>"},{"location":"app/overview/#features","title":"Features","text":"<p>The web console provides a UI for working with the platform without touching YAML or <code>kubectl</code>. From the browser you can:</p> <ul> <li>Submit new jobs by choosing a container image, GPU slice, storage size and inputs/secrets.</li> <li>Inspect runs, see their status, drill into logs, view and download their outputs.</li> <li>Create and manage volumes used by jobs.</li> <li>Create, list and delete secrets that are later injected into Pods as environment variables.</li> <li>Explore near real-time views of GPU usage and MIG layouts, powered by the telemetry sent by <code>walk:ai nos</code>.</li> </ul>"},{"location":"app/prerequisites/","title":"WIP","text":""},{"location":"cli/configuration/","title":"WIP","text":""},{"location":"cli/installation/","title":"WIP","text":""},{"location":"cli/overview/","title":"Command-line tool","text":"<p>The <code>walk:ai CLI</code> is an opinionated companion to the platform that focuses on two things: turning Python projects into container images, and interacting with <code>walk:ai</code> from the terminal.</p> <p>Features</p> <ul> <li>Builds OCI images from Python projects without writing Dockerfiles or Kubernetes manifests.</li> <li>Uses a <code>[tool.walkai]</code> section in <code>pyproject.toml</code> to declare entrypoint, OS dependencies, and ignore paths.</li> <li>Integrates with the <code>walk:ai</code> API to submit jobs using those images, passing inputs, secrets and GPU requirements.</li> </ul> <p>For installation, configuration examples, and detailed usage, see the CLI repository and README:</p> <ul> <li>GitHub: walkai-org/walkai-cli</li> </ul>"},{"location":"cli/prerequisites/","title":"WIP","text":""},{"location":"concepts/gpu_partitioner/","title":"GPU Partitioner","text":"<p>v1 <pre><code>flowchart TD\n    1[\"Pod is created/updated\"] --&gt; 2{\"Pod pending, unscheduled, and unschedulable?\"}\n\n    2 --\"no\"--&gt; R1[\"Return\"]\n    2 --\"yes\"--&gt; 3[\"Extract requested MIG profiles\"]\n\n    3 --&gt; 4{\"Any MIG profiles requested?\"}\n    4 --\"no\"--&gt; R2[\"Return\"]\n    4 --\"yes\"--&gt; 5[\"Profiles already present in the node?\"]\n\n    5 --\"yes\"--&gt; R4[\"Return\"]\n    5 --\"no\"--&gt; 6[\"Plan new geometry \"]\n\n    6 --&gt; 7{\"Feasible geometry found?\"}\n\n    7 --\"no\"--&gt; R5[\"Return\"]\n    7 --\"yes\"--&gt; 8{\"Is the new planned geometry different than the current one?\"}\n\n    8 --\"yes\"--&gt; 9[\"Patch node spec annotations + plan ID\"]\n    9 --&gt; R6[\"Return\"]\n\n    8 --\"no\"--&gt; R7[\"Return\"]\n</code></pre></p> <p>v2 <pre><code>flowchart TD\n    1[\"Start\"] --&gt; 2[\"Collecting pending, unscheduled and unschedulable pods\"]\n    2 --&gt; 3{\"None?\"}\n\n    3 --\"yes\"--&gt; W[\"Wait\"] --&gt; 1\n    3 --\"no\"--&gt;4[\"Update batch window timers\"]\n\n    4 --&gt; 5{\"Any MIG profiles requested?\"}\n\n    5 --\"no\"--&gt; W\n    5 --\"yes\"--&gt; 6[\"Profiles already present in the node?\"]\n\n    6 --\"yes\"--&gt; W\n    6 --\"no\"--&gt; 7[\"Plan new geometry\"]\n\n    7--&gt;8{\"Feasible geometry found?\"}\n\n    8 --\"no\"--&gt; W\n    8 --\"yes\"--&gt; 9{\"Is the new planned geometry different than the current one?\"}\n\n    9 --\"yes\"--&gt; 10[\"Patch node spec annotations + plan ID\"]\n    10--&gt; W\n\n    9 --\"no\"--&gt; W\n</code></pre> v3 <pre><code>flowchart TD\n    1[\"Start\"] --&gt; 2[\"Collecting pending, unscheduled and unschedulable pods\"]\n    2 --&gt; 3{\"None?\"}\n\n    3 --\"yes\"--&gt; W[\"Wait\"] --&gt; 1\n    3 --\"no\"--&gt;4.1[\"Update batch window timers\"]\n\n    4.1 --&gt;4.2[\"Sort pods by priority, then age\"]\n\n    4.2 --&gt; 5{\"Any MIG profiles requested?\"}\n\n    5 --\"no\"--&gt; W\n    5 --\"yes\"--&gt; 6[\"Profiles already present in the node?\"]\n\n    6 --\"yes\"--&gt; W\n    6 --\"no\"--&gt; 7[\"Plan new geometry, satisfying pod requests in priority order\"]\n\n    7--&gt;8{\"Feasible geometry found?\"}\n\n    8 --\"no\"--&gt; W\n    8 --\"yes\"--&gt; 9{\"Is the new planned geometry different than the current one?\"}\n\n    9 --\"yes\"--&gt; 10[\"Patch node spec annotations + plan ID\"]\n    10--&gt; W\n\n    9 --\"no\"--&gt; W\n</code></pre></p>"},{"location":"concepts/jobs/","title":"Jobs","text":"<p>In <code>walk:ai</code>, a Job is the main unit of work you submit to the platform. It describes what should run on the cluster, with which resources, and where outputs should go.</p> <p>A Job is eventually turned into a Kubernetes Job plus supporting resources, but the platform hides most of those details.</p>"},{"location":"concepts/jobs/#what-a-job-contains","title":"What a Job contains","text":"<p>When you create a Job (from the web application or the CLI), you typically specify:</p> <ul> <li>Container image: the image that will be executed. This is usually built with the <code>walkai</code> CLI from a Python project.</li> <li>GPU partition: the fraction of GPU you want for the Job (for example <code>1g.10gb</code>, <code>3g.40gb</code>, etc.), which is enforced by <code>walk:ai nos</code> using MIG.</li> <li>Storage for outputs: how much space the Job should have available for writing results. This is mounted inside the container and later uploaded to object storage.</li> <li>Inputs: references to input data already stored in object storage that should be made available to the Job.</li> <li>Secrets: a list of platform-managed secrets that will be injected as environment variables.</li> </ul> <p>The application takes this specification and turns it into a Kubernetes Job with the right volumes, environment and GPU requests.</p>"},{"location":"concepts/jobs/#job-lifecycle","title":"Job lifecycle","text":"<p>From the user\u2019s perspective, a Job goes through a simple lifecycle:</p> <ol> <li>Submitted \u2013 the Job is created via the API, CLI or web console.</li> <li>Pending \u2013 the platform is waiting for enough capacity (CPU, memory, GPU slice) to schedule it.</li> <li>Running \u2013 the container is executing on a node, using the requested GPU partition and storage.</li> <li>Completed / Failed \u2013 the main container exits successfully or with an error.</li> </ol> <p>At the end of the execution:</p> <ul> <li>Outputs written to the Job\u2019s output directory are collected by a sidecar and uploaded to object storage.</li> <li>Logs from the main container are also collected and stored so they can be inspected later from the application.</li> </ul>"},{"location":"concepts/jobs/#jobs-runs-and-scheduling","title":"Jobs, runs and scheduling","text":"<p>Internally, a Job can have one or more runs associated with it (for example, if you re-run the same Job with the same definition). Each run represents one concrete execution on the cluster, with its own status, start time and outputs.</p> <p>Scheduling decisions \u2014 including how GPU slices are partitioned and which Jobs are admitted first \u2014 are handled by the platform together with <code>walk:ai nos</code>. Future sections (for example Priorities &amp; scheduling) describe how concepts like priority, fairness and preemption affect how Jobs progress through the lifecycle.</p>"},{"location":"concepts/mig_agent/","title":"MIG Agent","text":"<p><pre><code>flowchart TD\nStart[\"Start migagent binary\"] --&gt; Config[\"Load config + NODE_NAME; setup manager/indexer\"]\nConfig --&gt; Init[\"Init NVML+MIG client; ensure at least one MIG GPU; delete unused MIG resources\"]\nInit --&gt; ReporterCtl[\"Start Reporter controller (watches Node resources, refreshInterval)\"]\nInit --&gt; ActuatorCtl[\"Start Actuator controller (watches Node annotations)\"]\n</code></pre> <pre><code>flowchart TD\nsubgraph Reporter\ndirection TB\nR0[\"Trigger: Node resource change or refresh interval\"] --&gt; R2[\"Query MIG client for devices; split free/used\"]\nR2 --&gt; R3[\"Build status annotations from devices\"]\nR3 --&gt; R4{\"Status unchanged AND AnnotationReportedPartitioningPlan == lastParsedPlanId?\"}\nR4 -- yes --&gt; R9[\"RequeueAfter(refreshInterval)\"]\nR4 -- no --&gt; R5[\"Patch Node: drop old MIG status annotations; set new status; set AnnotationReportedPartitioningPlan\"]\nR5 --&gt; R9\nR9 --&gt; R6[\"Exit\"]\nend\n</code></pre></p> <pre><code>flowchart TD\nsubgraph Actuator\ndirection TB\nA0[\"Trigger: Node annotation change\"] --&gt; A1{\"Has there been a report since last apply?\"}\nA1 -- no --&gt; A2[\"RequeueAfter(1s)\"]\nA1 -- yes --&gt; A4[\"Fetch Node; store lastParsedPlanId from AnnotationPartitioningPlan\"]\nA4 --&gt; A5[\"Parse spec annotations vs status annotations\"]\nA5 --&gt; A6{\"Spec matches status?\"}\nA6 -- yes --&gt; A20[\"Exit\"]\nA6 -- no --&gt; A7[\"Get MIG devices; build MigState\"]\nA7 --&gt; A70{\"Fetch error?\"}\nA70 -- NotFound --&gt; A71[\"Restart NVIDIA device plugin; exit\"]\nA70 -- Other error --&gt; A72[\"Return error\"]\nA70 -- Success --&gt; A73{\"MigState matches spec?\"}\nA73 -- yes --&gt; A20\nA73 -- no --&gt; A9[\"Build MigConfigPlan (delete extras, create missing, recreate free when creating)\"]\nA9 --&gt; A10{\"Plan empty?\"}\nA10 -- yes --&gt; A20\nA10 -- no --&gt; A11{\"Plan == lastAppliedPlan AND status unchanged?\"}\nA11 -- yes --&gt; A20\nA11 -- no --&gt; A12[\"Apply delete ops (free resources only); note restart need &amp; deleted profiles\"]\nA12 --&gt; A13[\"Apply create ops\"]\nA13 --&gt; A131{\"Success?\"} \nA131 -- no --&gt; A132[\"rollback deleted profiles on failure\"]\nA131 --yes--&gt; A14{\"Restart NVIDIA device plugin required?\"}\nA14 -- yes --&gt; A15[\"Restart device plugin\"]\nA14 -- no --&gt; A16[\"Skip restart\"]\nA15 --&gt; A17{\"Any operation error?\"}\nA16 --&gt; A17\nA17 -- error --&gt; A18[\"Return error\"]\nA17 -- success --&gt; A19[\"Call OnApplyDone\"] --&gt; A20\nend\n</code></pre>"},{"location":"nos/configuration/","title":"Configuration","text":"<p>You can customize the GPU Partitioner settings by editing the values file of the nos Helm chart. In this section we focus on some of the values that you would typically want to customize.</p>"},{"location":"nos/configuration/#requeue-interval","title":"Requeue interval","text":"<p>The GPU partitioner periodically scans all pending, unschedulable pods that request MIG resources and evaluates whether a different partitioning would make them schedulable. You can control how often this happens with:</p> <ul> <li><code>requeueIntervalSeconds</code>: how often the controller wakes up even if no new Pod events occur.</li> </ul> <p>Shorter intervals react faster to changes (for example when the scheduler no longer emits events) at the cost of more frequent reconciliation cycles. Longer intervals reduce churn but defer partitioning updates.</p>"},{"location":"nos/configuration/#available-mig-geometries","title":"Available MIG geometries","text":"<p>The GPU Partitioner determines the most proper partitioning plan to apply by considering the possible MIG geometries allowed for each of the GPU models present in the cluster.</p> <p>You can set the MIG geometries supported by each GPU model by editing the <code>gpuPartitioner.knownMigGeometries</code> value of the installation chart.</p> <p>You can edit this file to add new MIG geometries for new GPU models, or to edit the existing ones according to your specific needs. For instance, you can remove some MIG geometries if you don't want to allow them to be used for a certain GPU model.</p>"},{"location":"nos/configuration/#how-it-works","title":"How it works","text":"<p>The GPU Partitioner component watches for pending pods that cannot be scheduled due to lack of MIG resources they request. If it finds such pods, it checks the current partitioning state of the GPUs in the cluster and tries to find a new partitioning state that would allow to schedule them without deleting any of the used resources, taking into account the constraints presented by each GPU model's allowed MIG geometries.</p>"},{"location":"nos/configuration/#mig-partitioning","title":"MIG Partitioning","text":"<p>The actual partitioning specified by the GPU Partitioner for MIG GPUs is performed by the MIG Agent, which is a daemonset running on every node labeled with <code>nos.nebuly.com/gpu-partitioning: mig</code> that creates/deletes MIG profiles as requested by the GPU Partitioner.</p> <p>The MIG Agent exposes to the GPU Partitioner the used/free MIG resources of all the GPUs of the node on which it is running through the following node annotations:</p> <ul> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-free: &lt;quantity&gt;</code></li> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-used: &lt;quantity&gt;</code></li> </ul> <p>The MIG Agent also watches the node's annotations and, every time the desired MIG partitioning specified by the GPU Partitioner does not match the current state, it tries to apply it by creating and deleting the MIG profiles on the target GPUs. The GPU Partitioner specifies the desired MIG geometry of the GPUs of a node through annotations in the following format:</p> <p><code>nos.nebuly.com/spec-gpu-&lt;index&gt;-&lt;mig-profile&gt;: &lt;quantity&gt;</code></p> <p>Note that in some cases the MIG Agent might not be able to apply the desired MIG geometry specified by the GPU Partitioner. This can happen for two reasons:</p> <ol> <li>the MIG Agent never deletes MIG resources being in use by a Pod</li> <li>some MIG geometries require the MIG profiles to be created in a certain order, and due to reason (1) the MIG Agent might not be able to delete and re-create the existing MIG profiles in the order required by the new MIG geometry.</li> </ol> <p>In these cases, the MIG Agent tries to apply the desired partitioning and if it fails it rolls-back to its previous state.</p> <p>For further information regarding NVIDIA MIG and its integration with Kubernetes, please refer to the NVIDIA MIG User Guide and to the MIG Support in Kubernetes official documentation provided by NVIDIA.</p>"},{"location":"nos/helm-charts-README/","title":"nos","text":"<p>The open-source platform for running AI workloads on k8s in an optimized way, both in terms of hardware utilization and workload performance.</p>"},{"location":"nos/helm-charts-README/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/walkai-org/walkai-nos</li> </ul>"},{"location":"nos/helm-charts-README/#priority-awareness","title":"Priority Awareness","text":"<p>The chart can provision four PriorityClasses for GPU workloads: <code>nos-priority-low</code> (0), <code>nos-priority-medium</code> (1000, global default), <code>nos-priority-high</code> (2000), and <code>nos-priority-extra-high</code> (3000). Preemption is disabled on all of them. The GPU partitioner plans MIG repartitioning in priority order (age tie-breaker) and will not plan lower-priority pods if a higher-priority pod cannot be satisfied. You can disable or override these classes via the <code>priorityClasses</code> values.</p>"},{"location":"nos/helm-charts-README/#values","title":"Values","text":"Key Type Default Description allowDefaultNamespace bool <code>false</code> If true allows to deploy <code>nos</code> chart in the <code>default</code> namespace clusterInfoExporter.affinity object <code>{}</code> Affinity rules for the Cluster Info Exporter Pods. clusterInfoExporter.apiClient object - Configuration of the API client namespace, ServiceAccount, RBAC settings and token Secret. clusterInfoExporter.apiClient.adminClusterRoleName string <code>\"admin\"</code> ClusterRole granted by the admin RoleBinding. clusterInfoExporter.apiClient.adminRoleBindingName string <code>\"walkai-api-client-admin\"</code> Name of the RoleBinding that grants the admin ClusterRole inside the API client namespace. clusterInfoExporter.apiClient.createNamespace bool <code>true</code> Whether the chart should create the API client namespace. clusterInfoExporter.apiClient.discoveryClusterRoleBindingName string <code>\"discovery-minimal-for-walkai-api-client\"</code> Name of the ClusterRoleBinding that assigns the discovery ClusterRole. clusterInfoExporter.apiClient.discoveryClusterRoleName string <code>\"discovery-minimal\"</code> Name of the discovery ClusterRole granted to the API client. clusterInfoExporter.apiClient.enabled bool <code>true</code> Enable or disable provisioning of the API client resources. clusterInfoExporter.apiClient.namespace string <code>\"walkai\"</code> Namespace where the API client ServiceAccount and token Secret live. clusterInfoExporter.apiClient.serviceAccountName string <code>\"api-client\"</code> Name of the API client ServiceAccount. clusterInfoExporter.apiClient.tokenSecretName string <code>\"api-client-permanent-token\"</code> Name of the long-lived ServiceAccount token Secret for the API client. clusterInfoExporter.config.endpoint string <code>\"\"</code> API endpoint that receives cluster information payloads (exporter deploys only when set). clusterInfoExporter.config.httpTimeout string <code>\"10s\"</code> HTTP timeout for report requests. clusterInfoExporter.config.interval string <code>\"10s\"</code> Interval between reports (e.g. 10s, 5m). clusterInfoExporter.enabled bool <code>false</code> Enable or disable the Cluster Info Exporter DaemonSet (requires <code>clusterInfoExporter.config.endpoint</code>). clusterInfoExporter.fullnameOverride string <code>\"\"</code> Overrides the fully qualified name of the Cluster Info Exporter resources. clusterInfoExporter.image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy of the Cluster Info Exporter container. clusterInfoExporter.image.repository string <code>\"ghcr.io/walkai-org/nos-cluster-info-exporter\"</code> Repository of the Cluster Info Exporter image. clusterInfoExporter.image.tag string <code>\"\"</code> Overrides the Cluster Info Exporter image tag whose default is the chart appVersion. clusterInfoExporter.nameOverride string <code>\"\"</code> Overrides the name of the Cluster Info Exporter resources. clusterInfoExporter.nodeSelector object <code>{}</code> Node selector for the Cluster Info Exporter Pods. clusterInfoExporter.podAnnotations object <code>{}</code> Annotations added to the Cluster Info Exporter Pods. clusterInfoExporter.podSecurityContext object <code>{\"runAsNonRoot\":true}</code> Pod security context for the Cluster Info Exporter Pods. clusterInfoExporter.resources object <code>{\"limits\":{\"cpu\":\"200m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}</code> Resource requests and limits for the Cluster Info Exporter container. clusterInfoExporter.secret.apiToken string <code>\"\"</code> Value used when creating the API token Secret (ignored when an existing Secret is used). clusterInfoExporter.secret.create bool <code>false</code> When true the chart creates the Secret that stores the API token. clusterInfoExporter.secret.existingSecret string <code>\"\"</code> Use an already existing Secret instead of creating a new one. clusterInfoExporter.secret.key string <code>\"apiToken\"</code> Key that stores the token inside the Secret. clusterInfoExporter.secret.name string <code>\"cluster-info-exporter-secrets\"</code> Name of the Secret that stores the API token. clusterInfoExporter.securityContext object <code>{\"allowPrivilegeEscalation\":false,\"readOnlyRootFilesystem\":true}</code> Container security context for the Cluster Info Exporter. clusterInfoExporter.serviceAccount.annotations object <code>{}</code> Extra annotations added to the Cluster Info Exporter ServiceAccount. clusterInfoExporter.serviceAccount.create bool <code>true</code> Whether to create the Cluster Info Exporter ServiceAccount. clusterInfoExporter.serviceAccount.name string <code>\"\"</code> Overrides the Cluster Info Exporter ServiceAccount name. clusterInfoExporter.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Exists\"},{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/control-plane\",\"operator\":\"Exists\"}]</code> Tolerations applied to the Cluster Info Exporter Pods. gpuPartitioner.affinity object <code>{}</code> Sets the affinity config of the GPU Partitioner Pod. gpuPartitioner.requeueIntervalSeconds int <code>10</code> Interval at which the GPU partitioner reconciler wakes up even without new Pod events gpuPartitioner.devicePlugin.config.name string <code>\"nos-device-plugin-configs\"</code> Name of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the value \"devicePlugin.config.name\" of the Helm chart used for deploying the NVIDIA GPU Operator. gpuPartitioner.devicePlugin.config.namespace string <code>\"nebuly-nvidia\"</code> Namespace of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the namespace where the Nebuly NVIDIA Device Plugin has been deployed to. gpuPartitioner.devicePlugin.configUpdateDelaySeconds int <code>5</code> Duration of the delay between when the new partitioning config is computed and when it is sent to the NVIDIA device plugin. Since the config is provided to the plugin as a mounted ConfigMap, this delay is required to ensure that the updated ConfigMap is propagated to the mounted volume. gpuPartitioner.enabled bool <code>true</code> Enable or disable the <code>nos gpu partitioner</code> gpuPartitioner.fullnameOverride string <code>\"\"</code> gpuPartitioner.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the GPU Partitioner Docker image pull policy. gpuPartitioner.image.repository string <code>\"ghcr.io/walkai-org/nos-gpu-partitioner\"</code> Sets the GPU Partitioner Docker image. gpuPartitioner.image.tag string <code>\"\"</code> Overrides the GPU Partitioner image tag whose default is the chart appVersion. gpuPartitioner.knownMigGeometries list - List that associates GPU models to the respective allowed MIG configurations gpuPartitioner.kubeRbacProxy object - Configuration of the Kube RBAC Proxy, which runs as sidecar of all the GPU Partitioner components Pods. gpuPartitioner.leaderElection.enabled bool <code>true</code> Enables/Disables the leader election of the GPU Partitioner controller manager. gpuPartitioner.logLevel int <code>0</code> The level of log of the GPU Partitioner. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent object - Configuration of the MIG Agent component of the GPU Partitioner. gpuPartitioner.migAgent.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the MIG Agent Docker image pull policy. gpuPartitioner.migAgent.image.repository string <code>\"ghcr.io/walkai-org/nos-mig-agent\"</code> Sets the MIG Agent Docker image. gpuPartitioner.migAgent.image.tag string <code>\"\"</code> Overrides the MIG Agent image tag whose default is the chart appVersion. gpuPartitioner.migAgent.logLevel int <code>0</code> The level of log of the MIG Agent. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent.reportConfigIntervalSeconds int <code>10</code> Interval at which the mig-agent will report to k8s the MIG partitioning status of the GPUs of the Node gpuPartitioner.migAgent.resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Sets the resource requests and limits of the MIG Agent container. gpuPartitioner.migAgent.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}]</code> Sets the tolerations of the MIG Agent Pod. gpuPartitioner.nameOverride string <code>\"\"</code> gpuPartitioner.nodeSelector object <code>{}</code> Sets the nodeSelector config of the GPU Partitioner Pod. gpuPartitioner.podAnnotations object <code>{}</code> Sets the annotations of the GPU Partitioner Pod. gpuPartitioner.podSecurityContext object <code>{\"runAsNonRoot\":true,\"runAsUser\":1000}</code> Sets the security context of the GPU partitioner Pod. gpuPartitioner.replicaCount int <code>1</code> Number of replicas of the gpu-manager Pod. gpuPartitioner.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}}</code> Sets the resource limits and requests of the GPU partitioner container. gpuPartitioner.scheduler.config.name string <code>\"nos-scheduler-config\"</code> Name of the ConfigMap containing the k8s scheduler configuration file. If not specified or the ConfigMap does not exist, the GPU partitioner will use the default k8s scheduler profile. gpuPartitioner.tolerations list <code>[]</code> Sets the tolerations of the GPU Partitioner Pod. nvidiaGpuResourceMemoryGB int <code>32</code> Defines how many GB of memory each nvidia.com/gpu resource has. shareTelemetry bool <code>true</code> If true, shares with Nebuly telemetry data collected only during the Chart installation"},{"location":"nos/installation/","title":"Installation","text":"<p><code>nos</code> is distributed as a Helm 3 chart.</p> <p>If you want <code>nos</code> to send cluster insights to the <code>walk:ai</code> backend (or your own API), create a <code>values.yml</code> file and enable the telemetry exporter:</p> <p><pre><code>clusterInfoExporter:\n    config:\n      enabled: true\n      endpoint: https://api.example.com/cluster/insights\n      interval: 30s\n      httpTimeout: 15s\n    secret:\n      create: true\n      apiToken: \"&lt;your-token&gt;\"\n</code></pre> Then install the chart, pointing Helm at your values.yml:</p> <pre><code>helm install oci://ghcr.io/walkai-org/helm-charts/nos \\                                                                      \n  --version 0.0.9 \\\n  --namespace nos-system \\\n  --generate-name \\\n  --create-namespace \\\n  -f values.yml\n</code></pre> <p>If you don't need the telemetry module, you can omit <code>-f values.yml</code> and the <code>ClusterInfoExporter</code> daemonset will not be deployed.</p> <p>You can find all the available configuration values in the Chart documentation.</p>"},{"location":"nos/multi-instance-gpu/","title":"Multi-instance GPU (MIG)","text":"<p>Multi-instance GPU (MIG) is a technology available on NVIDIA Ampere and newer architectures that allows a single physical GPU to be securely partitioned into separate GPU instances for CUDA applications, each fully isolated with its own high-bandwidth memory, cache and compute cores.</p> <p>The isolated GPU slices are called MIG devices, and they are named adopting a format that indicates the compute and memory resources of the device. For example, 2g.20gb corresponds to a GPU slice with 20 GB of memory.</p>"},{"location":"nos/multi-instance-gpu/#limitations-of-mig","title":"Limitations of MIG","text":"<p>MIG is the GPU sharing approach that offers the strongest isolation between workloads, but it comes with an important trade-off: flexibility.</p> <p>First, MIG does not allow arbitrary slice sizes or counts. Each GPU model only supports a fixed set of MIG profiles, which limits how granularly you can partition the device. </p> <p>This also makes the configuration structurally rigid: at any point in time, the profiles you can schedule are constrained by the current layout of the GPU.</p> <p>For example, if an H100 is configured as: - <code>4g.40gb x 1</code> - <code>1g.10gb x 3</code></p> <p>and a Pod requests a <code>2g.20gb</code> profile, that Pod cannot be scheduled, even if the GPU is completely idle, because there is no compatible slice in the current configuration. It would only be schedulable after reconfiguring the device to include a <code>2g.20gb</code> profile.</p> <p>Second, with the tools that NVIDIA provides today, such as NVIDIA GPU Operator or mig-parted, all running workloads have to be evicted in order to change the current configuration. </p> <p>This makes dynamic reconfiguration disruptive and hard to use in practice.</p>"},{"location":"nos/multi-instance-gpu/#how-nos-improves-mig-based-scheduling","title":"How <code>nos</code> improves MIG-based scheduling","text":"<p><code>nos</code> is designed to mitigate these limitations. Through our MIG Agent, it can dynamically reconfigure a GPU\u2019s unused MIG slices at runtime, without touching slices that are currently in use. This allows the cluster to adapt the MIG layout to incoming workloads while keeping existing jobs running undisturbed.</p> <p>You can find out more on how MIG technology works in the official NVIDIA MIG User Guide.</p>"},{"location":"nos/overview/","title":"nos","text":"<p><code>walk:ai nos</code> is the in-cluster component of the platform. It runs on your Kubernetes cluster and is responsible for two main tasks: dynamically partitioning GPUs so that Pods can share them efficiently, and exporting GPU and workload telemetry that powers the <code>walk:ai</code> application.</p>"},{"location":"nos/overview/#origin-and-architecture","title":"Origin and architecture","text":"<p><code>walk:ai nos</code> is a fork of <code>nos</code> by nebuly-ai. We reuse its MIG Agent and GPU Partitioner components and extend them to support dynamic reconfiguration of MIG based on the current workloads. On top of that, we add our own telemetry pipeline that continuously publishes cluster and GPU state to the <code>walk:ai</code> backend.</p>"},{"location":"nos/overview/#dynamic-gpu-partitioning","title":"Dynamic GPU partitioning","text":"<p><code>walk:ai nos</code> allows Pods to request fractions of a physical GPU. Instead of assigning entire devices to a single Pod, GPUs are automatically split into MIG slices that can be requested by individual containers (for example, <code>1g.10gb</code>, <code>3g.40gb</code>, etc.). This lets multiple Pods share the same GPU and increases overall utilization.</p> <p>The partitioning is adjusted in real time:</p> <ul> <li><code>walk:ai</code> continuously watches pending Pods that request GPU fractions.</li> <li>Given the current and pending workload, the GPU Partitioner computes the best MIG configuration it can apply to fit as many of those Pods as possible.</li> <li>The MIG Agent then applies that configuration on the nodes, creating or tearing down MIG instances as needed.</li> </ul> <p>Under the hood, the GPU partitioning is implemented using NVIDIA Multi-Instance GPU (MIG), with runtime reconfiguration and scheduling improvements</p>"},{"location":"nos/overview/#telemetry-and-cluster-visibility","title":"Telemetry and cluster visibility","text":"<p>Beyond partitioning GPUs, <code>walk:ai nos</code> also acts as the telemetry layer of the platform. The in-cluster agents:</p> <ul> <li>Collect low-level GPU metrics (utilization, memory usage, MIG layout, allocations per Pod).</li> <li>Observe Jobs, Pods and their GPU requests.</li> <li>Periodically push a summarized view of this state to an api endpoint you configure during installation.</li> </ul> <p>This funcionality is opt-in. If you do not declare the configuration related to it, the daemonset will not be deployed.</p>"},{"location":"nos/prerequisites/","title":"Prerequisites","text":"<ol> <li>Kubernetes version 1.23 or newer</li> <li>NVIDIA GPU Operator</li> <li>Cert Manager (optional, but recommended)</li> </ol>"},{"location":"nos/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>Before installing <code>nos</code>, you must enable GPU support in your Kubernetes cluster, and we recommend using the NVIDIA GPU Operator for that.</p> <p>Create a <code>values.yml</code> file with the required tolerations for the <code>nos.nebuly.com/repartitioning=planned:NoSchedule</code> taint used during GPU repartitioning. You can start from values.yml and customize it for your cluster.</p> <p>You can install the NVIDIA GPU Operator as follows:</p> <pre><code>helm install --wait --generate-name \\\n     -n gpu-operator --create-namespace \\\n     nvidia/gpu-operator --version v22.9.0 \\\n     -f values.yml\n</code></pre> <p>If you set <code>driver.enabled=true</code> in the values file, the GPU Operator will automatically install a recent version of NVIDIA Drivers and CUDA on all the GPU-enabled nodes of your cluster, so you don't have to manually install them.</p> <p>For further information you can refer to the NVIDIA GPU Operator Documentation.</p>"},{"location":"nos/troubleshooting/","title":"Troubleshooting","text":"<p>If you run into issues with Automatic GPU Partitioning, you can troubleshoot by checking the logs of the GPU Partitioner and MIG Agent pods. You can do that by running the following commands:</p> <p>Check GPU Partitioner logs:</p> <pre><code> kubectl logs -n nos-system -l app.kubernetes.io/component=nos-gpu-partitioner -f\n</code></pre> <p>Check MIG Agent logs:</p> <pre><code> kubectl logs -n nos-system -l app.kubernetes.io/component=nos-mig-agent -f\n</code></pre>"}]}