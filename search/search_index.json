{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p><code>walk:ai</code> is an open-source platform for running AI/ML workloads on Kubernetes in an intuitive and optimized way, focused on increasing GPU utilization.</p> <p>Currently, the platform consists of three componentes:</p> <ul> <li> <p>nos: a set of services deployed in your cluster that handle dynamic GPU partitioning and send GPU and workload telemetry back to the <code>walk:ai</code> application.</p> </li> <li> <p>Application: a single control plane to submit jobs, inspect runs, manage volumes and secrets, and observe cluster GPU usage through a browser-based UI.</p> </li> <li> <p>Command-line tool: a developer-friendly CLI that turns Python projects into container images without writing Dockerfiles or Kubernetes manifests. Also lets you interface with the <code>walk:ai</code> platform just like in the browser, but without leaving your terminal.</p> </li> </ul>"},{"location":"overview/","title":"Overview","text":"<p><code>walk:ai</code> is an open-source platform for running AI/ML workloads on Kubernetes in an intuitive and optimized way, focused on increasing GPU utilization.</p> <p>Currently, the platform consists of three componentes:</p> <ul> <li> <p>nos: a set of services deployed in your cluster that handle dynamic GPU partitioning and send GPU and workload telemetry back to the <code>walk:ai</code> application.</p> </li> <li> <p>Application: a single control plane to submit jobs, inspect runs, manage volumes and secrets, and observe cluster GPU usage through a browser-based UI.</p> </li> <li> <p>Command-line tool: a developer-friendly CLI that turns Python projects into container images without writing Dockerfiles or Kubernetes manifests. Also lets you interface with the <code>walk:ai</code> platform just like in the browser, but without leaving your terminal.</p> </li> </ul> <p></p>"},{"location":"vpn/","title":"Vpn","text":"<ol> <li>Crear resource group en region deseada</li> <li>Crear una virtual network en AZ, eligiendo su espacio de direcciones</li> <li>Crear una subnet, con un subespacio de direcciones.</li> <li>Crear otra subnet, de tipo Gateway, con otro subespacio, mas peque\u00f1o, pero no menor a /27. </li> <li> <p>Crear virtual network gateway</p> <ul> <li>Buscar en search, es la de marketplace.</li> <li>Hay que usar la subnet de tipo Gateway</li> </ul> </li> <li> <p>Crear VPC en AWS</p> <ul> <li>Crear subnets</li> <li>Crear route tables</li> </ul> </li> <li>Crear customer gateway en AWS utilizando la IP p\u00fablica de la virtual network gateway previa.</li> <li>Crear Virtual Private Gateway<ul> <li>Attach a la VPC creada previamente.</li> </ul> </li> <li>Crear Site-to-Site VPN<ul> <li>Tenes que asignarle la customer gateway de (7)</li> <li>La Virtual Private Gateway de (8)</li> <li>En static routes hay que poner la subnet de (3) (subnet de la VM)</li> </ul> </li> </ol>"},{"location":"app/configuration/","title":"Configuration","text":"<p>After creating your repository from the template, configure the GitHub Actions variables and secrets in: <code>Settings</code> -&gt; <code>Secrets and variables</code> -&gt; <code>Actions</code>.</p> <p>Note</p> <p>Workflows authenticate to AWS by assuming the role referenced in the <code>AWS_DEPLOY_ROLE_ARN</code> secret, so make sure it matches the IAM role created during installation.</p>"},{"location":"app/configuration/#variables","title":"Variables","text":"<ul> <li><code>AWS_S3_TERRAFORM_BUCKET</code>: name of the S3 bucket defined in the prerequisites for Terraform state.</li> <li><code>BOOTSTRAP_FIRST_USER</code>: email address of the initial admin user.</li> <li><code>VPC_CIDR</code>: address space for the VPC. Default is <code>172.31.0.0/16</code>.</li> <li><code>K8S_CLUSTER_URL</code>: URL of the Kubernetes cluster.</li> <li><code>BASE_DOMAIN</code>: your registered web domain.</li> <li><code>EXTERNAL_DNS</code>: <code>true</code> if the domain is already managed externally, <code>false</code> if the root domain must be managed by this deployment.</li> <li><code>SMTP_HOST</code>: hostname of your SMTP provider (for example, <code>smtp.azurecomm.net</code>).</li> <li><code>SMTP_PORT</code>: SMTP port that supports STARTTLS (commonly <code>587</code>, but some providers use <code>25</code> or <code>2525</code>).</li> <li><code>SMTP_USERNAME</code>: SMTP username (for example, <code>apikey</code> or <code>user@mycompany.com</code>).</li> <li><code>MAIL_FROM</code>: sender shown in emails (recommended format: <code>Name &lt;email@domain&gt;</code>).</li> </ul>"},{"location":"app/configuration/#secrets","title":"Secrets","text":"<ul> <li><code>AWS_DEPLOY_ROLE_ARN</code>: ARN of the IAM role created during installation.</li> <li><code>K8S_CLUSTER_TOKEN</code>: token used to access the Kubernetes cluster.</li> <li><code>SMTP_PASSWORD</code>: password or token for <code>SMTP_USERNAME</code>.</li> </ul>"},{"location":"app/deployment/","title":"Deployment","text":"<p>After configuring all repository variables and secrets, run the GitHub Actions workflow <code>Apply Infra (Bootstrap)</code>. To run a workflow, go to the Actions tab, select it, click Run workflow, and confirm the run.</p> <p>Once it finishes, open the job named <code>show_name_servers</code> and copy the four name server values it prints. Before continuing, you must add those name servers in your domain manager (if you already use one) under the <code>walkai</code> subdomain of your <code>BASE_DOMAIN</code>. If you manage the domain at the registrar, add them there instead.</p> <p>When the name servers are in place, run the <code>Apply Infra (Finalize HTTPS)</code> workflow the same way as before.</p> <p>Finally, check the inbox for <code>BOOTSTRAP_FIRST_USER</code> and follow the instructions in that email to complete the setup.</p> <p>Tip</p> <p>If you need to tear down the infrastructure, you can run the <code>Destroy Infra</code> workflow.</p>"},{"location":"app/installation/","title":"Installation","text":"<p>The application infrastructure is deployed from a Terraform-based GitHub template repository.</p> <ol> <li>Open the walkai-infra template repository.</li> <li>Click Use this template to create your own repository based on it, and name it as you prefer.</li> <li>In AWS IAM, create a new role for GitHub Actions and set the trust relationship policy using this JSON.</li> <li>Before creating the trust policy, replace <code>&lt;your-aws-account-id&gt;</code> and <code>&lt;org&gt;/&lt;repo&gt;</code> with your AWS account ID and the GitHub repository you created in step 2.</li> <li>Create a custom policy from this JSON.</li> <li>Before creating the policy, replace <code>&lt;your-terraform-state-bucket&gt;</code> in the first two statements with the S3 bucket name from the prerequisites.</li> <li>Attach the custom policy to the role. The policy follows the principle of least privilege, granting only the permissions needed by the deployment pipeline.</li> <li>Save the role ARN. You will set it as the <code>AWS_DEPLOY_ROLE_ARN</code> secret in the configuration.</li> </ol> <p>You will use this repository in the next sections to configure GitHub Actions and run the deployment workflows.</p>"},{"location":"app/overview/","title":"Application","text":"<p>The <code>walk:ai application</code> is the control plane of the platform. It exposes an HTTP API and a browser-based console to submit and monitor jobs, manage data, and inspect how GPUs are being used across the cluster.</p>"},{"location":"app/overview/#features","title":"Features","text":"<p>The web console provides a UI for working with the platform without touching YAML or <code>kubectl</code>. From the browser you can:</p> <ul> <li>Submit new jobs by choosing a container image, GPU slice, storage size and inputs/secrets.</li> <li>Inspect runs, see their status, drill into logs, view and download their outputs.</li> <li>Create and manage volumes used by jobs.</li> <li>Create, list and delete secrets that are later injected into Pods as environment variables.</li> <li>Explore near real-time views of GPU usage and MIG layouts, powered by the telemetry sent by <code>walk:ai nos</code>.</li> </ul>"},{"location":"app/prerequisites/","title":"Prerequisites","text":"<p>Before running the automatic deployment pipeline, make sure you have:</p> <ol> <li>An SMTP email service with STARTTLS and username/password authentication.</li> <li>A registered web domain (for example, <code>mycompany.com</code>).</li> <li>A GitHub account.</li> <li>An AWS account.</li> <li>An S3 bucket in that AWS account to store the Terraform state.</li> <li>An IAM OpenID Connect identity provider for GitHub Actions (<code>token.actions.githubusercontent.com</code>) in that AWS account.</li> </ol> <p>The identity provider is created once per AWS account. Follow the official GitHub guide for AWS OIDC setup: Configuring OpenID Connect in AWS.</p> <p>During installation, you will use this provider to create the deployment role and reference the S3 bucket name in its policy.</p>"},{"location":"cli/configuration/","title":"Configuration","text":"<p>Every project you build must declare a [tool.walkai] section inside its pyproject.toml.</p> <pre><code>[tool.walkai]\nentrypoint = \"python -m app.main\"\nos_dependencies = [\"git\", \"gettext\", \"cron\"]\nignore = [\"datasets/sample.csv\"]\n</code></pre> <ul> <li>entrypoint (required) is the command that will run when the container starts.</li> <li>os_dependencies (optional) is a list of Debian packages to install in the image. The default Heroku builder synthesises a project.toml describing these dependencies so the deb-packages buildpack can install them.</li> <li>ignore (optional) is a list of files or directories that walkai should exclude from the container image (useful for large datasets you plan to mount separately).</li> </ul>"},{"location":"cli/installation/","title":"Installation","text":"<p>You can install it using <code>uv</code> referencing the repo:</p> <pre><code>uv tool install git+https://github.com/walkai-org/walkai-cli\n</code></pre> <p>Or you can install it as editable:</p> <pre><code>uv tool install --editable .\n</code></pre>"},{"location":"cli/overview/","title":"Command-line tool","text":"<p>The <code>walk:ai CLI</code> is an opinionated companion to the platform that focuses on two things: turning Python projects into container images, and interacting with <code>walk:ai</code> from the terminal.</p> <p>Features</p> <ul> <li>Builds OCI images from Python projects without writing Dockerfiles or Kubernetes manifests.</li> <li>Uses a <code>[tool.walkai]</code> section in <code>pyproject.toml</code> to declare entrypoint, OS dependencies, and ignore paths.</li> <li>Integrates with the <code>walk:ai</code> API to submit jobs using those images, passing inputs, secrets and GPU requirements.</li> </ul> <p>Note</p> <p>For detailed usage references, see the CLI repository and README</p>"},{"location":"cli/prerequisites/","title":"Prerequisites","text":"<ol> <li>Python 3.13+</li> <li>uv</li> <li>pack</li> <li>A container client (docker or podman)</li> </ol>"},{"location":"nos/configuration/","title":"Configuration","text":"<p>You can customize the GPU Partitioner settings by editing the values file of the nos Helm chart. In this section we focus on some of the values that you would typically want to customize.</p>"},{"location":"nos/configuration/#requeue-interval","title":"Requeue interval","text":"<p>The GPU partitioner periodically scans all pending, unschedulable pods that request MIG resources and evaluates whether a different partitioning would make them schedulable. You can control how often this happens with:</p> <ul> <li><code>requeueIntervalSeconds</code>: how often the controller wakes up even if no new Pod events occur.</li> </ul> <p>Note</p> <p>Shorter intervals react faster to changes (for example when the scheduler no longer emits events) at the cost of more frequent reconciliation cycles. Longer intervals reduce churn but defer partitioning updates.</p>"},{"location":"nos/configuration/#priority-awareness","title":"Priority Awareness","text":"<p>The chart can provision four PriorityClasses for GPU workloads: <code>nos-priority-low</code> (0), <code>nos-priority-medium</code> (1000, global default), <code>nos-priority-high</code> (2000), and <code>nos-priority-extra-high</code> (3000). Preemption is disabled on all of them. The GPU partitioner plans MIG repartitioning in priority order (age tie-breaker) and will not plan lower-priority pods if a higher-priority pod cannot be satisfied. You can disable or override these classes via the <code>priorityClasses</code> values.</p>"},{"location":"nos/configuration/#available-mig-geometries","title":"Available MIG geometries","text":"<p>The GPU Partitioner determines the most proper partitioning plan to apply by considering the possible MIG geometries allowed for each of the GPU models present in the cluster.</p> <p>You can set the MIG geometries supported by each GPU model by editing the <code>gpuPartitioner.knownMigGeometries</code> value of the installation chart.</p> <p>You can edit this file to add new MIG geometries for new GPU models, or to edit the existing ones according to your specific needs. For instance, you can remove some MIG geometries if you don't want to allow them to be used for a certain GPU model.</p>"},{"location":"nos/configuration/#how-it-works","title":"How it works","text":"<p>The GPU Partitioner component watches for pending pods that cannot be scheduled due to lack of MIG resources they request. If it finds such pods, it checks the current partitioning state of the GPUs in the cluster and tries to find a new partitioning state that would allow to schedule them without deleting any of the used resources, taking into account the constraints presented by each GPU model's allowed MIG geometries.</p>"},{"location":"nos/configuration/#mig-partitioning","title":"MIG Partitioning","text":"<p>The actual partitioning specified by the GPU Partitioner for MIG GPUs is performed by the MIG Agent, which is a daemonset running on every node labeled with <code>nos.nebuly.com/gpu-partitioning: mig</code> that creates/deletes MIG profiles as requested by the GPU Partitioner.</p> <p>The MIG Agent exposes to the GPU Partitioner the used/free MIG resources of all the GPUs of the node on which it is running through the following node annotations:</p> <ul> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-free: &lt;quantity&gt;</code></li> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-used: &lt;quantity&gt;</code></li> </ul> <p>The MIG Agent also watches the node's annotations and, every time the desired MIG partitioning specified by the GPU Partitioner does not match the current state, it tries to apply it by creating and deleting the MIG profiles on the target GPUs. The GPU Partitioner specifies the desired MIG geometry of the GPUs of a node through annotations in the following format:</p> <p><code>nos.nebuly.com/spec-gpu-&lt;index&gt;-&lt;mig-profile&gt;: &lt;quantity&gt;</code></p> <p>In some cases the MIG Agent might not be able to apply the desired MIG geometry specified by the GPU Partitioner. This can happen for two reasons:</p> <ol> <li>the MIG Agent never deletes MIG resources being in use by a Pod</li> <li>some MIG geometries require the MIG profiles to be created in a certain order, and due to reason (1) the MIG Agent might not be able to delete and re-create the existing MIG profiles in the order required by the new MIG geometry.</li> </ol> <p>In these cases, the MIG Agent tries to apply the desired partitioning and if it fails it rolls-back to its previous state.</p> <p>Note</p> <p>For further information regarding NVIDIA MIG and its integration with Kubernetes, please refer to the NVIDIA MIG User Guide and to the MIG Support in Kubernetes official documentation provided by NVIDIA.</p>"},{"location":"nos/helm-charts-README/","title":"nos","text":"<p>The open-source platform for running AI workloads on k8s in an optimized way, both in terms of hardware utilization and workload performance.</p>"},{"location":"nos/helm-charts-README/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/walkai-org/walkai-nos</li> </ul>"},{"location":"nos/helm-charts-README/#values","title":"Values","text":"Key Type Default Description allowDefaultNamespace bool <code>false</code> If true allows to deploy <code>nos</code> chart in the <code>default</code> namespace clusterInfoExporter.affinity object <code>{}</code> Affinity rules for the Cluster Info Exporter Pods. clusterInfoExporter.apiClient object - Configuration of the API client namespace, ServiceAccount, RBAC settings and token Secret. clusterInfoExporter.apiClient.adminClusterRoleName string <code>\"admin\"</code> ClusterRole granted by the admin RoleBinding. clusterInfoExporter.apiClient.adminRoleBindingName string <code>\"walkai-api-client-admin\"</code> Name of the RoleBinding that grants the admin ClusterRole inside the API client namespace. clusterInfoExporter.apiClient.createNamespace bool <code>true</code> Whether the chart should create the API client namespace. clusterInfoExporter.apiClient.discoveryClusterRoleBindingName string <code>\"discovery-minimal-for-walkai-api-client\"</code> Name of the ClusterRoleBinding that assigns the discovery ClusterRole. clusterInfoExporter.apiClient.discoveryClusterRoleName string <code>\"discovery-minimal\"</code> Name of the discovery ClusterRole granted to the API client. clusterInfoExporter.apiClient.enabled bool <code>true</code> Enable or disable provisioning of the API client resources. clusterInfoExporter.apiClient.namespace string <code>\"walkai\"</code> Namespace where the API client ServiceAccount and token Secret live. clusterInfoExporter.apiClient.serviceAccountName string <code>\"api-client\"</code> Name of the API client ServiceAccount. clusterInfoExporter.apiClient.tokenSecretName string <code>\"api-client-permanent-token\"</code> Name of the long-lived ServiceAccount token Secret for the API client. clusterInfoExporter.config.endpoint string <code>\"\"</code> API endpoint that receives cluster information payloads (exporter deploys only when set). clusterInfoExporter.config.httpTimeout string <code>\"10s\"</code> HTTP timeout for report requests. clusterInfoExporter.config.interval string <code>\"10s\"</code> Interval between reports (e.g. 10s, 5m). clusterInfoExporter.enabled bool <code>false</code> Enable or disable the Cluster Info Exporter DaemonSet (requires <code>clusterInfoExporter.config.endpoint</code>). clusterInfoExporter.fullnameOverride string <code>\"\"</code> Overrides the fully qualified name of the Cluster Info Exporter resources. clusterInfoExporter.image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy of the Cluster Info Exporter container. clusterInfoExporter.image.repository string <code>\"ghcr.io/walkai-org/nos-cluster-info-exporter\"</code> Repository of the Cluster Info Exporter image. clusterInfoExporter.image.tag string <code>\"\"</code> Overrides the Cluster Info Exporter image tag whose default is the chart appVersion. clusterInfoExporter.nameOverride string <code>\"\"</code> Overrides the name of the Cluster Info Exporter resources. clusterInfoExporter.nodeSelector object <code>{}</code> Node selector for the Cluster Info Exporter Pods. clusterInfoExporter.podAnnotations object <code>{}</code> Annotations added to the Cluster Info Exporter Pods. clusterInfoExporter.podSecurityContext object <code>{\"runAsNonRoot\":true}</code> Pod security context for the Cluster Info Exporter Pods. clusterInfoExporter.resources object <code>{\"limits\":{\"cpu\":\"200m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}</code> Resource requests and limits for the Cluster Info Exporter container. clusterInfoExporter.secret.apiToken string <code>\"\"</code> Value used when creating the API token Secret (ignored when an existing Secret is used). clusterInfoExporter.secret.create bool <code>false</code> When true the chart creates the Secret that stores the API token. clusterInfoExporter.secret.existingSecret string <code>\"\"</code> Use an already existing Secret instead of creating a new one. clusterInfoExporter.secret.key string <code>\"apiToken\"</code> Key that stores the token inside the Secret. clusterInfoExporter.secret.name string <code>\"cluster-info-exporter-secrets\"</code> Name of the Secret that stores the API token. clusterInfoExporter.securityContext object <code>{\"allowPrivilegeEscalation\":false,\"readOnlyRootFilesystem\":true}</code> Container security context for the Cluster Info Exporter. clusterInfoExporter.serviceAccount.annotations object <code>{}</code> Extra annotations added to the Cluster Info Exporter ServiceAccount. clusterInfoExporter.serviceAccount.create bool <code>true</code> Whether to create the Cluster Info Exporter ServiceAccount. clusterInfoExporter.serviceAccount.name string <code>\"\"</code> Overrides the Cluster Info Exporter ServiceAccount name. clusterInfoExporter.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Exists\"},{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/control-plane\",\"operator\":\"Exists\"}]</code> Tolerations applied to the Cluster Info Exporter Pods. gpuPartitioner.affinity object <code>{}</code> Sets the affinity config of the GPU Partitioner Pod. gpuPartitioner.requeueIntervalSeconds int <code>10</code> Interval at which the GPU partitioner reconciler wakes up even without new Pod events gpuPartitioner.devicePlugin.config.name string <code>\"nos-device-plugin-configs\"</code> Name of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the value \"devicePlugin.config.name\" of the Helm chart used for deploying the NVIDIA GPU Operator. gpuPartitioner.devicePlugin.config.namespace string <code>\"nebuly-nvidia\"</code> Namespace of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the namespace where the Nebuly NVIDIA Device Plugin has been deployed to. gpuPartitioner.devicePlugin.configUpdateDelaySeconds int <code>5</code> Duration of the delay between when the new partitioning config is computed and when it is sent to the NVIDIA device plugin. Since the config is provided to the plugin as a mounted ConfigMap, this delay is required to ensure that the updated ConfigMap is propagated to the mounted volume. gpuPartitioner.enabled bool <code>true</code> Enable or disable the <code>nos gpu partitioner</code> gpuPartitioner.fullnameOverride string <code>\"\"</code> gpuPartitioner.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the GPU Partitioner Docker image pull policy. gpuPartitioner.image.repository string <code>\"ghcr.io/walkai-org/nos-gpu-partitioner\"</code> Sets the GPU Partitioner Docker image. gpuPartitioner.image.tag string <code>\"\"</code> Overrides the GPU Partitioner image tag whose default is the chart appVersion. gpuPartitioner.knownMigGeometries list - List that associates GPU models to the respective allowed MIG configurations gpuPartitioner.kubeRbacProxy object - Configuration of the Kube RBAC Proxy, which runs as sidecar of all the GPU Partitioner components Pods. gpuPartitioner.leaderElection.enabled bool <code>true</code> Enables/Disables the leader election of the GPU Partitioner controller manager. gpuPartitioner.logLevel int <code>0</code> The level of log of the GPU Partitioner. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent object - Configuration of the MIG Agent component of the GPU Partitioner. gpuPartitioner.migAgent.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the MIG Agent Docker image pull policy. gpuPartitioner.migAgent.image.repository string <code>\"ghcr.io/walkai-org/nos-mig-agent\"</code> Sets the MIG Agent Docker image. gpuPartitioner.migAgent.image.tag string <code>\"\"</code> Overrides the MIG Agent image tag whose default is the chart appVersion. gpuPartitioner.migAgent.logLevel int <code>0</code> The level of log of the MIG Agent. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent.reportConfigIntervalSeconds int <code>10</code> Interval at which the mig-agent will report to k8s the MIG partitioning status of the GPUs of the Node gpuPartitioner.migAgent.resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Sets the resource requests and limits of the MIG Agent container. gpuPartitioner.migAgent.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}]</code> Sets the tolerations of the MIG Agent Pod. gpuPartitioner.nameOverride string <code>\"\"</code> gpuPartitioner.nodeSelector object <code>{}</code> Sets the nodeSelector config of the GPU Partitioner Pod. gpuPartitioner.podAnnotations object <code>{}</code> Sets the annotations of the GPU Partitioner Pod. gpuPartitioner.podSecurityContext object <code>{\"runAsNonRoot\":true,\"runAsUser\":1000}</code> Sets the security context of the GPU partitioner Pod. gpuPartitioner.replicaCount int <code>1</code> Number of replicas of the gpu-manager Pod. gpuPartitioner.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}}</code> Sets the resource limits and requests of the GPU partitioner container. gpuPartitioner.scheduler.config.name string <code>\"nos-scheduler-config\"</code> Name of the ConfigMap containing the k8s scheduler configuration file. If not specified or the ConfigMap does not exist, the GPU partitioner will use the default k8s scheduler profile. gpuPartitioner.tolerations list <code>[]</code> Sets the tolerations of the GPU Partitioner Pod. nvidiaGpuResourceMemoryGB int <code>32</code> Defines how many GB of memory each nvidia.com/gpu resource has. shareTelemetry bool <code>true</code> If true, shares with Nebuly telemetry data collected only during the Chart installation"},{"location":"nos/installation/","title":"Installation","text":"<p><code>nos</code> is distributed as a Helm 3 chart.</p> <p>If you want <code>nos</code> to send cluster insights to the <code>walk:ai</code> backend (or your own API), create a <code>values.yml</code> file and enable the telemetry exporter:</p> <pre><code>clusterInfoExporter:\n    enabled: true\n    config:\n      endpoint: https://api.example.com/cluster/insights\n      interval: 3s\n      httpTimeout: 5s\n    secret:\n      create: true\n      apiToken: \"&lt;your-token&gt;\"\ngpuPartitioner:\n  migAgent:\n      reportConfigIntervalSeconds: 3\n  preemption:\n    enabled: true\n</code></pre> <p>Then install the chart, pointing Helm at your values.yml:</p> <pre><code>helm install oci://ghcr.io/walkai-org/helm-charts/nos \\\n  --version 0.0.9 \\\n  --namespace nos-system \\\n  --generate-name \\\n  --create-namespace \\\n  -f values.yml\n</code></pre> <p>Note</p> <p>If you don't need the telemetry module, you can omit <code>-f values.yml</code> and the <code>ClusterInfoExporter</code> daemonset will not be deployed.</p> <p>Note</p> <p>You can find all the available configuration values in the Chart documentation.</p> <p>After that, you can fetch the token the application uses for its cluster configuration: <pre><code>kubectl get secret api-client-permanent-token -n walkai   -o jsonpath='{.data.token}' | base64 -d; echo\n</code></pre></p>"},{"location":"nos/multi-instance-gpu/","title":"Multi-instance GPU (MIG)","text":"<p>Multi-instance GPU (MIG) is a technology available on NVIDIA Ampere and newer architectures that allows a single physical GPU to be securely partitioned into separate GPU instances for CUDA applications, each fully isolated with its own high-bandwidth memory, cache and compute cores.</p> <p>The isolated GPU slices are called MIG devices, and they are named adopting a format that indicates the compute and memory resources of the device. For example, 2g.20gb corresponds to a GPU slice with 20 GB of memory.</p>"},{"location":"nos/multi-instance-gpu/#limitations-of-mig","title":"Limitations of MIG","text":"<p>MIG is the GPU sharing approach that offers the strongest isolation between workloads, but it comes with an important trade-off: flexibility.</p> <p>First, MIG does not allow arbitrary slice sizes or counts. Each GPU model only supports a fixed set of MIG profiles, which limits how granularly you can partition the device. </p> <p>This also makes the configuration structurally rigid: at any point in time, the profiles you can schedule are constrained by the current layout of the GPU.</p> <p>For example, if an H100 is configured as: - <code>4g.40gb x 1</code> - <code>1g.10gb x 3</code></p> <p>and a Pod requests a <code>2g.20gb</code> profile, that Pod cannot be scheduled, even if the GPU is completely idle, because there is no compatible slice in the current configuration. It would only be schedulable after reconfiguring the device to include a <code>2g.20gb</code> profile.</p> <p>Second, with the tools that NVIDIA provides today, such as NVIDIA GPU Operator or mig-parted, all running workloads have to be evicted in order to change the current configuration. </p> <p>This makes dynamic reconfiguration disruptive and hard to use in practice.</p>"},{"location":"nos/multi-instance-gpu/#how-nos-improves-mig-based-scheduling","title":"How <code>nos</code> improves MIG-based scheduling","text":"<p><code>nos</code> is designed to mitigate these limitations. Through our MIG Agent, it can dynamically reconfigure a GPU\u2019s unused MIG slices at runtime, without touching slices that are currently in use. This allows the cluster to adapt the MIG layout to incoming workloads while keeping existing jobs running undisturbed.</p> <p>Note</p> <p>You can find out more on how MIG technology works in the official NVIDIA MIG User Guide.</p>"},{"location":"nos/overview/","title":"nos","text":"<p><code>walk:ai nos</code> is the in-cluster component of the platform. It runs on your Kubernetes cluster and is responsible for two main tasks: dynamically partitioning GPUs so that Pods can share them efficiently, and exporting GPU and workload telemetry that powers the <code>walk:ai</code> application.</p>"},{"location":"nos/overview/#origin-and-architecture","title":"Origin and architecture","text":"<p><code>walk:ai nos</code> is a fork of <code>nos</code> by nebuly-ai. We reuse its MIG Agent and GPU Partitioner components and extend them to support dynamic reconfiguration of MIG based on the current workloads. On top of that, we add our own telemetry pipeline that continuously publishes cluster and GPU state to the <code>walk:ai</code> backend.</p>"},{"location":"nos/overview/#dynamic-gpu-partitioning","title":"Dynamic GPU partitioning","text":"<p><code>walk:ai nos</code> allows Pods to request fractions of a physical GPU. Instead of assigning entire devices to a single Pod, GPUs are automatically split into MIG slices that can be requested by individual containers (for example, <code>1g.10gb</code>, <code>3g.40gb</code>, etc.). This lets multiple Pods share the same GPU and increases overall utilization.</p> <p>The partitioning is adjusted in real time:</p> <ul> <li><code>walk:ai</code> continuously watches pending Pods that request GPU fractions.</li> <li>Given the current and pending workload, the GPU Partitioner computes the best MIG configuration it can apply to fit as many of those Pods as possible.</li> <li>The MIG Agent then applies that configuration on the nodes, creating or tearing down MIG instances as needed.</li> </ul> <p>Note</p> <p>Under the hood, the GPU partitioning is implemented using NVIDIA Multi-Instance GPU (MIG), with runtime reconfiguration and scheduling improvements</p>"},{"location":"nos/overview/#telemetry-and-cluster-visibility","title":"Telemetry and cluster visibility","text":"<p>Beyond partitioning GPUs, <code>walk:ai nos</code> also acts as the telemetry layer of the platform. The in-cluster agents:</p> <ul> <li>Collect low-level GPU metrics (utilization, memory usage, MIG layout, allocations per Pod).</li> <li>Observe Jobs, Pods and their GPU requests.</li> <li>Periodically push a summarized view of this state to an api endpoint you configure during installation.</li> </ul> <p>Note</p> <p>This funcionality is opt-in. If you do not declare the configuration related to it, the daemonset will not be deployed.</p>"},{"location":"nos/prerequisites/","title":"Prerequisites","text":"<ol> <li>Kubernetes version 1.23 or newer</li> <li>MIG enabled on your GPU</li> <li>NVIDIA GPU Operator</li> <li>Cert Manager (optional, but recommended)</li> </ol>"},{"location":"nos/prerequisites/#enable-mig","title":"Enable MIG","text":"<p>Enable MIG mode on your GPU:</p> <pre><code>sudo nvidia-smi -i 0 -mig 1\n</code></pre> <p>Verify that MIG mode is active:</p> <pre><code>nvidia-smi -i 0 -q | grep -A3 \"MIG Mode\"\n</code></pre> <p>Note</p> <p>A reboot may be required for the change to take effect.</p>"},{"location":"nos/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>Before installing <code>nos</code>, you must enable GPU support in your Kubernetes cluster, and we recommend using the NVIDIA GPU Operator for that.</p> <p>Create a <code>values.yml</code> file with the required tolerations for the <code>nos.nebuly.com/repartitioning=planned:NoSchedule</code> taint used during GPU repartitioning. You can start from values.yml and customize it for your cluster.</p> <p>You can install the NVIDIA GPU Operator as follows:</p> <pre><code>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia &amp;&amp; helm repo update\nhelm install --wait --generate-name \\\n     -n gpu-operator --create-namespace \\\n     nvidia/gpu-operator --version v22.9.0 \\\n     -f values.yml\n</code></pre> <p>If you set <code>driver.enabled=true</code> in the values file, the GPU Operator will automatically install a recent version of NVIDIA Drivers and CUDA on all the GPU-enabled nodes of your cluster, so you don't have to manually install them.</p> <p>Note</p> <p>For further information you can refer to the NVIDIA GPU Operator Documentation.</p>"},{"location":"nos/troubleshooting/","title":"Troubleshooting","text":"<p>If you run into issues with Automatic GPU Partitioning, you can troubleshoot by checking the logs of the GPU Partitioner and MIG Agent pods. You can do that by running the following commands:</p> <p>Check GPU Partitioner logs:</p> <pre><code> kubectl logs -n nos-system -l app.kubernetes.io/component=nos-gpu-partitioner -f\n</code></pre> <p>Check MIG Agent logs:</p> <pre><code> kubectl logs -n nos-system -l app.kubernetes.io/component=nos-mig-agent -f\n</code></pre>"}]}