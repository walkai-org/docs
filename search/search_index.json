{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p><code>walk:ai</code> is an open-source platform for running AI/ML workloads on Kubernetes in an intuitive and optimized way, focused on increasing GPU utilization.</p> <p>Currently, the platform consists of three componentes:</p> <ul> <li> <p>nos: a set of services deployed in your cluster that handle dynamic GPU partitioning and send GPU and workload telemetry back to the <code>walk:ai</code> application.</p> </li> <li> <p>Application: a single control plane to submit jobs, inspect runs, manage volumes and secrets, and observe cluster GPU usage through a browser-based UI.</p> </li> <li> <p>Command-line tool: a developer-friendly CLI that turns Python projects into container images without writing Dockerfiles or Kubernetes manifests. Also lets you interface with the <code>walk:ai</code> platform just like in the browser, but without leaving your terminal.</p> </li> </ul>"},{"location":"overview/","title":"Overview","text":"<p><code>walk:ai</code> is an open-source platform for running AI/ML workloads on Kubernetes in an intuitive and optimized way, focused on increasing GPU utilization.</p> <p>Currently, the platform consists of three componentes:</p> <ul> <li> <p>nos: a set of services deployed in your cluster that handle dynamic GPU partitioning and send GPU and workload telemetry back to the <code>walk:ai</code> application.</p> </li> <li> <p>Application: a single control plane to submit jobs, inspect runs, manage volumes and secrets, and observe cluster GPU usage through a browser-based UI.</p> </li> <li> <p>Command-line tool: a developer-friendly CLI that turns Python projects into container images without writing Dockerfiles or Kubernetes manifests. Also lets you interface with the <code>walk:ai</code> platform just like in the browser, but without leaving your terminal.</p> </li> </ul> <p></p>"},{"location":"app/configuration/","title":"WIP","text":""},{"location":"app/installation/","title":"WIP","text":""},{"location":"app/overview/","title":"Application","text":"<p>The <code>walk:ai</code> application is the control plane of the platform. It exposes an HTTP API and a browser-based console to submit and monitor jobs, manage data, and inspect how GPUs are being used across the cluster.</p>"},{"location":"app/overview/#features","title":"Features","text":"<p>The web console provides a UI for working with the platform without touching YAML or <code>kubectl</code>. From the browser you can:</p> <ul> <li>Submit new jobs by choosing a container image, GPU slice, storage size and inputs/secrets.</li> <li>Inspect runs, see their status, drill into logs, view and download their outputs.</li> <li>Create and manage volumes used by jobs.</li> <li>Create, list and delete secrets that are later injected into Pods as environment variables.</li> <li>Explore near real-time views of GPU usage and MIG layouts, powered by the telemetry sent by <code>walk:ai nos</code>.</li> </ul>"},{"location":"app/prerequisites/","title":"WIP","text":""},{"location":"cli/configuration/","title":"WIP","text":""},{"location":"cli/installation/","title":"WIP","text":""},{"location":"cli/overview/","title":"Command-line tool","text":"<p>The <code>walkai</code> CLI is an opinionated companion to the platform that focuses on two things: turning Python projects into container images, and interacting with <code>walk:ai</code> from the terminal.</p> <p>Features</p> <ul> <li>Builds OCI images from Python projects without writing Dockerfiles or Kubernetes manifests.</li> <li>Uses a <code>[tool.walkai]</code> section in <code>pyproject.toml</code> to declare entrypoint, OS dependencies, and ignore paths.</li> <li>Integrates with the <code>walk:ai</code> API to submit jobs using those images, passing inputs, secrets and GPU requirements.</li> </ul> <p>For installation, configuration examples, and detailed usage, see the CLI repository and README:</p> <ul> <li>GitHub: walkai-org/walkai-cli</li> </ul>"},{"location":"cli/prerequisites/","title":"WIP","text":""},{"location":"concepts/jobs/","title":"Jobs","text":"<p>In <code>walk:ai</code>, a Job is the main unit of work you submit to the platform. It describes what should run on the cluster, with which resources, and where outputs should go.</p> <p>A Job is eventually turned into a Kubernetes Job plus supporting resources, but the platform hides most of those details.</p>"},{"location":"concepts/jobs/#what-a-job-contains","title":"What a Job contains","text":"<p>When you create a Job (from the web application or the CLI), you typically specify:</p> <ul> <li>Container image: the image that will be executed. This is usually built with the <code>walkai</code> CLI from a Python project.</li> <li>GPU partition: the fraction of GPU you want for the Job (for example <code>1g.10gb</code>, <code>3g.40gb</code>, etc.), which is enforced by <code>walk:ai nos</code> using MIG.</li> <li>Storage for outputs: how much space the Job should have available for writing results. This is mounted inside the container and later uploaded to object storage.</li> <li>Inputs: references to input data already stored in object storage that should be made available to the Job.</li> <li>Secrets: a list of platform-managed secrets that will be injected as environment variables.</li> </ul> <p>The application takes this specification and turns it into a Kubernetes Job with the right volumes, environment and GPU requests.</p>"},{"location":"concepts/jobs/#job-lifecycle","title":"Job lifecycle","text":"<p>From the user\u2019s perspective, a Job goes through a simple lifecycle:</p> <ol> <li>Submitted \u2013 the Job is created via the API, CLI or web console.</li> <li>Pending \u2013 the platform is waiting for enough capacity (CPU, memory, GPU slice) to schedule it.</li> <li>Running \u2013 the container is executing on a node, using the requested GPU partition and storage.</li> <li>Completed / Failed \u2013 the main container exits successfully or with an error.</li> </ol> <p>At the end of the execution:</p> <ul> <li>Outputs written to the Job\u2019s output directory are collected by a sidecar and uploaded to object storage.</li> <li>Logs from the main container are also collected and stored so they can be inspected later from the application.</li> </ul>"},{"location":"concepts/jobs/#jobs-runs-and-scheduling","title":"Jobs, runs and scheduling","text":"<p>Internally, a Job can have one or more runs associated with it (for example, if you re-run the same Job with the same definition). Each run represents one concrete execution on the cluster, with its own status, start time and outputs.</p> <p>Scheduling decisions \u2014 including how GPU slices are partitioned and which Jobs are admitted first \u2014 are handled by the platform together with <code>walk:ai nos</code>. Future sections (for example Priorities &amp; scheduling) describe how concepts like priority, fairness and preemption affect how Jobs progress through the lifecycle.</p>"},{"location":"nos/configuration/","title":"Configuration","text":"<p>You can customize the GPU Partitioner settings by editing the values file of the nos Helm chart. In this section we focus on some of the values that you would typically want to customize.</p>"},{"location":"nos/configuration/#pods-batch-size","title":"Pods batch size","text":"<p>The GPU partitioner processes pending pods in batches of configurable size. You can set the batch size by editing the following two parameters of the configuration:</p> <ul> <li><code>batchWindowTimeoutSeconds</code>: timeout of the time window used for batching pending Pods. The time window starts when the GPU Partitioner starts processing a batch of pending Pods, and ends when the timeout expires or the batch is completed.</li> <li><code>batchWindowIdleSeconds</code>: idle time before a batch of pods is considered completed. Once the time window of a batch starts, if idle time elapses and no new pending pods are detected during this time, the batch is considered completed.</li> </ul> <p>Increase the value of these two parameters if you want the GPU partitioner to take into account more pending Pods when deciding the GPU partitioning plan, thus making potentially it more effective.</p> <p>Set lower values if you want the partitioning to be performed more frequently (e.g. if you want to react faster to changes in the cluster), and you don't mind if the partitioning is less effective (e.g. the resources requested by some pending pods might not be created).</p>"},{"location":"nos/configuration/#scheduler-configuration","title":"Scheduler configuration","text":"<p>The GPU Partitioner uses an internal scheduler to simulate the scheduling of the pending pods to determine whether a candidate GPU partitioning plan would make the pending pods schedulable.</p> <p>The GPU Partitioner reads the scheduler configuration from the ConfigMap defined by the field <code>gpuPartitioner.scheduler.config</code>, and it falls back to the default configuration if the ConfigMap is not found. You can edit this field to provide your custom scheduler configuration.</p> <p>If you installed <code>nos</code> with the <code>scheduler</code> flag enabled, the GPU Partitioner will use its configuration unless you specify a custom ConfigMap.</p>"},{"location":"nos/configuration/#available-mig-geometries","title":"Available MIG geometries","text":"<p>The GPU Partitioner determines the most proper partitioning plan to apply by considering the possible MIG geometries allowed each of the GPU models present in the cluster.</p> <p>You can set the MIG geometries supported by each GPU model by editing the <code>gpuPartitioner.knownMigGeometries</code> value of the installation chart.</p> <p>You can edit this file to add new MIG geometries for new GPU models, or to edit the existing ones according to your specific needs. For instance, you can remove some MIG geometries if you don't want to allow them to be used for a certain GPU model.</p>"},{"location":"nos/configuration/#how-it-works","title":"How it works","text":"<p>The GPU Partitioner component watches for pending pods that cannot be scheduled due to lack of MIG/MPS resources they request. If it finds such pods, it checks the current partitioning state of the GPUs in the cluster and tries to find a new partitioning state that would allow to schedule them without deleting any of the used resources.</p> <p>It does that by using an internal k8s scheduler, so that before choosing a candidate partitioning, the GPU Partitioner simulates the scheduling to check whether the partitioning would actually allow to schedule the pending Pods. If multiple partitioning configuration can be used to schedule the pending Pods, the one that would result in the highest number of schedulable pods is chosen.</p> <p>Moreover, just in the case of MIG partitioning, each specific GPU model allows to create only certain combinations of MIG profiles, which are called MIG geometries, so the GPU partitioner takes this constraint into account when trying to find a new partitioning. The available MIG geometries of each GPU model are defined in the field <code>gpuPartitioner.knownMigGeometries</code> field of the Helm chart.</p>"},{"location":"nos/configuration/#mig-partitioning","title":"MIG Partitioning","text":"<p>The actual partitioning specified by the GPU Partitioner for MIG GPUs is performed by the MIG Agent, which is a daemonset running on every node labeled with <code>nos.nebuly.com/gpu-partitioning: mig</code> that creates/deletes MIG profiles as requested by the GPU Partitioner.</p> <p>The MIG Agent exposes to the GPU Partitioner the used/free MIG resources of all the GPUs of the node on which it is running through the following node annotations:</p> <ul> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-free: &lt;quantity&gt;</code></li> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-used: &lt;quantity&gt;</code></li> </ul> <p>The MIG Agent also watches the node's annotations and, every time there desired MIG partitioning specified by the GPU Partitioner does not match the current state, it tries to apply it by creating and deleting the MIG profiles on the target GPUs. The GPU Partitioner specifies the desired MIG geometry of the GPUs of a node through annotations in the following format:</p> <p><code>nos.nebuly.com/spec-gpu-&lt;index&gt;-&lt;mig-profile&gt;: &lt;quantity&gt;</code></p> <p>Note that in some cases the MIG Agent might not be able to apply the desired MIG geometry specified by the GPU Partitioner. This can happen for two reasons:</p> <ol> <li>the MIG Agent never deletes MIG resources being in use by a Pod</li> <li>some MIG geometries require the MIG profiles to be created in a certain order, and due to reason (1) the MIG Agent might not be able to delete and re-create the existing MIG profiles in the order required by the new MIG geometry.</li> </ol> <p>In these cases, the MIG Agent tries to apply the desired partitioning by creating as many required resources as possible, in order to maximize the number of schedulable Pods. This can result in the MIG Agent applying the desired MIG geometry only partially.</p> <p>For further information regarding NVIDIA MIG and its integration with Kubernetes, please refer to the NVIDIA MIG User Guide and to the MIG Support in Kubernetes official documentation provided by NVIDIA.</p>"},{"location":"nos/configuration/#mps-partitioning","title":"MPS Partitioning","text":"<p>The creation and deletion of MPS resources is handled by the k8s-device-plugin, which can expose a single GPU as multiple MPS resources according to its configuration.</p> <p>When allocating a container requesting an MPS resource, the device plugin takes care of injecting theenvironment variables and mounting the volumes required by the container to communicate to the MPS server, making sure that the resource limits defined by the device requested by the container are enforced.</p> <p>For more information about MPS integration with Kubernetes you can refer to the Nebuly k8s-device-plugin documentation.</p>"},{"location":"nos/helm-charts-README/","title":"nos","text":"<p>The open-source platform for running AI workloads on k8s in an optimized way, both in terms of hardware utilization and workload performance.</p>"},{"location":"nos/helm-charts-README/#maintainers","title":"Maintainers","text":"Name Email Url Michele Zanotti m.zanotti@nebuly.com Diego Fiori d.fiori@nebuly.com"},{"location":"nos/helm-charts-README/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/nebuly-ai/nos</li> </ul>"},{"location":"nos/helm-charts-README/#values","title":"Values","text":"Key Type Default Description allowDefaultNamespace bool <code>false</code> If true allows to deploy <code>nos</code> chart in the <code>default</code> namespace clusterInfoExporter.affinity object <code>{}</code> Affinity rules for the Cluster Info Exporter Pods. clusterInfoExporter.apiClient object - Configuration of the API client namespace, ServiceAccount, RBAC settings and token Secret. clusterInfoExporter.apiClient.adminClusterRoleName string <code>\"admin\"</code> ClusterRole granted by the admin RoleBinding. clusterInfoExporter.apiClient.adminRoleBindingName string <code>\"walkai-api-client-admin\"</code> Name of the RoleBinding that grants the admin ClusterRole inside the API client namespace. clusterInfoExporter.apiClient.createNamespace bool <code>true</code> Whether the chart should create the API client namespace. clusterInfoExporter.apiClient.discoveryClusterRoleBindingName string <code>\"discovery-minimal-for-walkai-api-client\"</code> Name of the ClusterRoleBinding that assigns the discovery ClusterRole. clusterInfoExporter.apiClient.discoveryClusterRoleName string <code>\"discovery-minimal\"</code> Name of the discovery ClusterRole granted to the API client. clusterInfoExporter.apiClient.enabled bool <code>true</code> Enable or disable provisioning of the API client resources. clusterInfoExporter.apiClient.namespace string <code>\"walkai\"</code> Namespace where the API client ServiceAccount and token Secret live. clusterInfoExporter.apiClient.serviceAccountName string <code>\"api-client\"</code> Name of the API client ServiceAccount. clusterInfoExporter.apiClient.tokenSecretName string <code>\"api-client-permanent-token\"</code> Name of the long-lived ServiceAccount token Secret for the API client. clusterInfoExporter.config.endpoint string <code>\"https://example.com/cluster/insights\"</code> API endpoint that receives cluster information payloads. clusterInfoExporter.config.httpTimeout string <code>\"10s\"</code> HTTP timeout for report requests. clusterInfoExporter.config.interval string <code>\"10s\"</code> Interval between reports (e.g. 10s, 5m). clusterInfoExporter.enabled bool <code>true</code> Enable or disable the Cluster Info Exporter DaemonSet. clusterInfoExporter.fullnameOverride string <code>\"\"</code> Overrides the fully qualified name of the Cluster Info Exporter resources. clusterInfoExporter.image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy of the Cluster Info Exporter container. clusterInfoExporter.image.repository string <code>\"ghcr.io/walkai-org/nos-cluster-info-exporter\"</code> Repository of the Cluster Info Exporter image. clusterInfoExporter.image.tag string <code>\"\"</code> Overrides the Cluster Info Exporter image tag whose default is the chart appVersion. clusterInfoExporter.nameOverride string <code>\"\"</code> Overrides the name of the Cluster Info Exporter resources. clusterInfoExporter.nodeSelector object <code>{}</code> Node selector for the Cluster Info Exporter Pods. clusterInfoExporter.podAnnotations object <code>{}</code> Annotations added to the Cluster Info Exporter Pods. clusterInfoExporter.podSecurityContext object <code>{\"runAsNonRoot\":true}</code> Pod security context for the Cluster Info Exporter Pods. clusterInfoExporter.resources object <code>{\"limits\":{\"cpu\":\"200m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"64Mi\"}}</code> Resource requests and limits for the Cluster Info Exporter container. clusterInfoExporter.secret.apiToken string <code>\"\"</code> Value used when creating the API token Secret (ignored when an existing Secret is used). clusterInfoExporter.secret.create bool <code>false</code> When true the chart creates the Secret that stores the API token. clusterInfoExporter.secret.existingSecret string <code>\"\"</code> Use an already existing Secret instead of creating a new one. clusterInfoExporter.secret.key string <code>\"apiToken\"</code> Key that stores the token inside the Secret. clusterInfoExporter.secret.name string <code>\"cluster-info-exporter-secrets\"</code> Name of the Secret that stores the API token. clusterInfoExporter.securityContext object <code>{\"allowPrivilegeEscalation\":false,\"readOnlyRootFilesystem\":true}</code> Container security context for the Cluster Info Exporter. clusterInfoExporter.serviceAccount.annotations object <code>{}</code> Extra annotations added to the Cluster Info Exporter ServiceAccount. clusterInfoExporter.serviceAccount.create bool <code>true</code> Whether to create the Cluster Info Exporter ServiceAccount. clusterInfoExporter.serviceAccount.name string <code>\"\"</code> Overrides the Cluster Info Exporter ServiceAccount name. clusterInfoExporter.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Exists\"},{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/control-plane\",\"operator\":\"Exists\"}]</code> Tolerations applied to the Cluster Info Exporter Pods. gpuPartitioner.affinity object <code>{}</code> Sets the affinity config of the GPU Partitioner Pod. gpuPartitioner.batchWindowIdleSeconds int <code>10</code> Idle seconds before the GPU partitioner processes the current batch if no new pending Pods are created, and the timeout has not been reached.  Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.batchWindowTimeoutSeconds int <code>60</code> Timeout of the window used by the GPU partitioner for batching pending Pods.  Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.devicePlugin.config.name string <code>\"nos-device-plugin-configs\"</code> Name of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the value \"devicePlugin.config.name\" of the Helm chart used for deploying the NVIDIA GPU Operator. gpuPartitioner.devicePlugin.config.namespace string <code>\"nebuly-nvidia\"</code> Namespace of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the namespace where the Nebuly NVIDIA Device Plugin has been deployed to. gpuPartitioner.devicePlugin.configUpdateDelaySeconds int <code>5</code> Duration of the delay between when the new partitioning config is computed and when it is sent to the NVIDIA device plugin. Since the config is provided to the plugin as a mounted ConfigMap, this delay is required to ensure that the updated ConfigMap is propagated to the mounted volume. gpuPartitioner.enabled bool <code>true</code> Enable or disable the <code>nos gpu partitioner</code> gpuPartitioner.fullnameOverride string <code>\"\"</code> gpuPartitioner.gpuAgent object - Configuration of the GPU Agent component of the GPU Partitioner. gpuPartitioner.gpuAgent.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the GPU Agent Docker image pull policy. gpuPartitioner.gpuAgent.image.repository string <code>\"ghcr.io/walkai-org/nos-gpu-agent\"</code> Sets the GPU Agent Docker image. gpuPartitioner.gpuAgent.image.tag string <code>\"\"</code> Overrides the GPU Agent image tag whose default is the chart appVersion. gpuPartitioner.gpuAgent.logLevel int <code>0</code> The level of log of the GPU Agent. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.gpuAgent.reportConfigIntervalSeconds int <code>10</code> Interval at which the mig-agent will report to k8s status of the GPUs of the Node gpuPartitioner.gpuAgent.resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Sets the resource requests and limits of the GPU Agent container. gpuPartitioner.gpuAgent.runtimeClassName string <code>nil</code> The container runtime class name to use for the GPU Agent container. gpuPartitioner.gpuAgent.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}]</code> Sets the tolerations of the GPU Agent Pod. gpuPartitioner.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the GPU Partitioner Docker image pull policy. gpuPartitioner.image.repository string <code>\"ghcr.io/walkai-org/nos-gpu-partitioner\"</code> Sets the GPU Partitioner Docker image. gpuPartitioner.image.tag string <code>\"\"</code> Overrides the GPU Partitioner image tag whose default is the chart appVersion. gpuPartitioner.knownMigGeometries list - List that associates GPU models to the respective allowed MIG configurations gpuPartitioner.kubeRbacProxy object - Configuration of the Kube RBAC Proxy, which runs as sidecar of all the GPU Partitioner components Pods. gpuPartitioner.leaderElection.enabled bool <code>true</code> Enables/Disables the leader election of the GPU Partitioner controller manager. gpuPartitioner.logLevel int <code>0</code> The level of log of the GPU Partitioner. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent object - Configuration of the MIG Agent component of the GPU Partitioner. gpuPartitioner.migAgent.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the MIG Agent Docker image pull policy. gpuPartitioner.migAgent.image.repository string <code>\"ghcr.io/walkai-org/nos-mig-agent\"</code> Sets the MIG Agent Docker image. gpuPartitioner.migAgent.image.tag string <code>\"\"</code> Overrides the MIG Agent image tag whose default is the chart appVersion. gpuPartitioner.migAgent.logLevel int <code>0</code> The level of log of the MIG Agent. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent.reportConfigIntervalSeconds int <code>10</code> Interval at which the mig-agent will report to k8s the MIG partitioning status of the GPUs of the Node gpuPartitioner.migAgent.resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Sets the resource requests and limits of the MIG Agent container. gpuPartitioner.migAgent.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}]</code> Sets the tolerations of the MIG Agent Pod. gpuPartitioner.nameOverride string <code>\"\"</code> gpuPartitioner.nodeSelector object <code>{}</code> Sets the nodeSelector config of the GPU Partitioner Pod. gpuPartitioner.podAnnotations object <code>{}</code> Sets the annotations of the GPU Partitioner Pod. gpuPartitioner.podSecurityContext object <code>{\"runAsNonRoot\":true,\"runAsUser\":1000}</code> Sets the security context of the GPU partitioner Pod. gpuPartitioner.replicaCount int <code>1</code> Number of replicas of the gpu-manager Pod. gpuPartitioner.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}}</code> Sets the resource limits and requests of the GPU partitioner container. gpuPartitioner.scheduler.config.name string <code>\"nos-scheduler-config\"</code> Name of the ConfigMap containing the k8s scheduler configuration file. If not specified or the ConfigMap does not exist, the GPU partitioner will use the default k8s scheduler profile. gpuPartitioner.tolerations list <code>[]</code> Sets the tolerations of the GPU Partitioner Pod. nvidiaGpuResourceMemoryGB int <code>32</code> Defines how many GB of memory each nvidia.com/gpu resource has. shareTelemetry bool <code>true</code> If true, shares with Nebuly telemetry data collected only during the Chart installation"},{"location":"nos/installation/","title":"Installation","text":"<p>Warning</p> <p>Before proceeding with <code>nos</code> installation, please make sure to meet the requirements  described in the Prerequisites page.</p> <p>You can install <code>nos</code> using Helm 3 (recommended). You can find all the available configuration values in the Chart documentation.</p> <pre><code>helm install oci://ghcr.io/walkai-org/helm-charts/nos \\                                                                      \n  --version 0.0.2 \\\n  --namespace nos-system \\\n  --generate-name \\\n  --create-namespace\n</code></pre> <p>Alternatively, you can use Kustomize by cloning the repository and running <code>make deploy</code>.</p>"},{"location":"nos/overview/","title":"nos","text":"<p><code>walk:ai nos</code> is the in-cluster component of the platform. It runs on your Kubernetes cluster and is responsible for two main tasks: dynamically partitioning GPUs so that Pods can share them efficiently, and exporting GPU and workload telemetry that powers the <code>walk:ai</code> application.</p>"},{"location":"nos/overview/#origin-and-architecture","title":"Origin and architecture","text":"<p><code>walk:ai nos</code> is a fork of <code>nos</code> by nebuly-ai. We reuse its MIG Agent and GPU Partitioner components (deployed as DaemonSets) and extend them to support dynamic, runtime reconfiguration of MIG based on the current workload. On top of that, we add our own telemetry pipeline that continuously publishes cluster and GPU state to the <code>walk:ai</code> backend.</p>"},{"location":"nos/overview/#dynamic-gpu-partitioning","title":"Dynamic GPU partitioning","text":"<p><code>walk:ai nos</code> allows Pods to request fractions of a physical GPU. Instead of assigning entire devices to a single Pod, GPUs are automatically split into MIG slices that can be requested by individual containers (for example, <code>1g.10gb</code>, <code>3g.40gb</code>, etc.). This lets multiple Pods share the same card and increases overall utilization.</p> <p>The partitioning is adjusted in real time:</p> <ul> <li><code>walk:ai</code> continuously watches pending Pods that request GPU fractions.</li> <li>Given the current and pending workload, it computes the best MIG configuration it can apply to fit as many of those Pods as possible.</li> <li>The MIG Agent and GPU Partitioner then apply that configuration on the nodes, creating or tearing down MIG instances as needed.</li> </ul> <p>Under the hood, the GPU partitioning is implemented using NVIDIA Multi-Instance GPU (MIG) and builds directly on the upstream <code>nos</code> MIG Agent and GPU Partitioner.</p>"},{"location":"nos/overview/#telemetry-and-cluster-visibility","title":"Telemetry and cluster visibility","text":"<p>Beyond partitioning GPUs, <code>walk:ai nos</code> also acts as the telemetry layer of the platform. The in-cluster agents:</p> <ul> <li>Collect low-level GPU metrics (utilization, memory usage, MIG layout, allocations per Pod).</li> <li>Observe Jobs, Pods and their GPU requests.</li> <li>Periodically push a summarized view of this state to the <code>walk:ai</code> backend, which stores it in a fast, query-optimized datastore.</li> </ul> <p>The web application then uses this data to render near real-time views of:</p> <ul> <li>How GPUs and MIG slices are currently allocated and utilized.</li> <li>Which Jobs and Pods are running or pending, and why.</li> <li>How dynamic partitioning decisions are affecting overall cluster efficiency.</li> </ul> <p>In this way, <code>walk:ai nos</code> is the bridge between Kubernetes and the higher-level platform: it enforces GPU partitioning decisions on the nodes, and it exposes a coherent picture of the cluster state so that users can understand and trust how their workloads are being scheduled.</p>"},{"location":"nos/partitioning-modes-comparison/","title":"Multi-instance GPU (MIG)","text":"<p>Multi-instance GPU (MIG) is a technology available on NVIDIA Ampere or more recent architectures that allows to securely partition a GPU into separate GPU instances for CUDA applications, each fully isolated with its own high-bandwidth memory, cache, and compute cores.</p> <p>The isolated GPU slices are called MIG devices, and they are named adopting a format that indicates the compute and memory resources of the device. For example, 2g.20gb corresponds to a GPU slice with 20 GB of memory.</p> <p>MIG does not allow to create GPU slices of custom sizes and quantity, as each GPU model only supports a specific set of MIG profiles. This reduces the degree of granularity with which you can partition the GPUs. Additionally, the MIG devices must be created respecting certain placement rules, which further limits flexibility of use.</p> <p>MIG is the GPU sharing approach that offers the highest level of isolation among processes. However, it lacks in flexibility and it is compatible only with few GPU architectures (Ampere and Hopper).</p> <p>You can find out more on how MIG technology works in the official NVIDIA MIG User Guide.</p>"},{"location":"nos/prerequisites/","title":"Prerequisites","text":"<ol> <li>Kubernetes version 1.23 or newer</li> <li>NVIDIA GPU Operator</li> <li>Cert Manager (optional, but recommended)</li> </ol>"},{"location":"nos/prerequisites/#nvidia-gpu-operator","title":"NVIDIA GPU Operator","text":"<p>Before installing <code>nos</code>, you must enable GPU support in your Kubernetes cluster, and we recommend using the NVIDIA GPU Operator for that</p> <p>You can install the NVIDIA GPU Operator as follows:</p> <pre><code>helm install --wait --generate-name \\\n     -n gpu-operator --create-namespace \\\n     nvidia/gpu-operator --version v22.9.0 \\\n     --set driver.enabled=true \\\n     --set migManager.enabled=false \\\n     --set mig.strategy=mixed \\\n     --set toolkit.enabled=true\n</code></pre> <p>Note that the GPU Operator will automatically install a recent version of NVIDIA Drivers and CUDA on all the GPU-enabled nodes of your cluster, so you don't have to manually install them.</p> <p>For further information you can refer to the NVIDIA GPU Operator Documentation.</p>"},{"location":"nos/troubleshooting/","title":"Troubleshooting","text":"<p>If you run into issues with Automatic GPU Partitioning, you can troubleshoot by checking the logs of the GPU Partitioner and MIG Agent pods. You can do that by running the following commands:</p> <p>Check GPU Partitioner logs:</p> <pre><code> kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-gpu-partitioner -f\n</code></pre> <p>Check MIG Agent logs:</p> <pre><code> kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-mig-agent -f\n</code></pre> <p>Check Nebuly's device-plugin logs:</p> <pre><code>kubectl logs -n nebuly-nvidia -l app.kubernetes.io/name=nebuly-nvidia-device-plugin -f\n</code></pre>"}]}